{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    return 0.5 * np.mean((y-tx@w) **2) \n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    for i in range(len(grid_w0)):\n",
    "        for j in range(len(grid_w1)):\n",
    "            losses[i,j] = compute_loss(y,tx,[grid_w0[i],grid_w1[j]])\n",
    "    # ***************************************************\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=109.17758755205352, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.011 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "\n",
    "    return - (( tx.T @(y - tx @ w)) ) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        \n",
    "        loss = compute_loss(y,tx,w)\n",
    "        grad = compute_gradient(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w  - gamma*grad\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.2367127591674, w0=51.30574540147361, w1=9.435798704492269\n",
      "GD iter. 1/49: loss=265.3024621089606, w0=66.69746902191571, w1=12.266538315839997\n",
      "GD iter. 2/49: loss=37.878379550441274, w0=71.31498610804834, w1=13.115760199244328\n",
      "GD iter. 3/49: loss=17.41021212017447, w0=72.70024123388814, w1=13.37052676426563\n",
      "GD iter. 4/49: loss=15.568077051450457, w0=73.11581777164007, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638305, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.3858879656522, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543453, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.385887869613667, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829403, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829403, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.3858878688294, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.034 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cdf4a9c1c042ecb69ecf1549999ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    return - (( tx.T @(y - tx @ w)) ) / y.shape[0]\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        grad = compute_gradient(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w  - gamma*grad\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2792.2367127591674, w0=7.329392200210517, w1=1.347971243498896\n",
      "SGD iter. 1/49: loss=2264.6350560300034, w0=13.925845180399985, w1=2.5611453626479053\n",
      "SGD iter. 2/49: loss=1837.2777140793803, w0=19.862652862570503, w1=3.653002069882012\n",
      "SGD iter. 3/49: loss=1491.1182670993758, w0=25.20577977652397, w1=4.63567310639271\n",
      "SGD iter. 4/49: loss=1210.7291150455724, w0=30.01459399908209, w1=5.520077039252339\n",
      "SGD iter. 5/49: loss=983.613901881991, w0=34.3425267993844, w1=6.316040578826005\n",
      "SGD iter. 6/49: loss=799.6505792194903, w0=38.23766631965648, w1=7.032407764442306\n",
      "SGD iter. 7/49: loss=650.6402878628646, w0=41.74329188790136, w1=7.6771382314969765\n",
      "SGD iter. 8/49: loss=529.9419518639979, w0=44.89835489932174, w1=8.25739565184618\n",
      "SGD iter. 9/49: loss=432.17629970491595, w0=47.73791160960008, w1=8.779627330160464\n",
      "SGD iter. 10/49: loss=352.98612145605955, w0=50.293512648850594, w1=9.24963584064332\n",
      "SGD iter. 11/49: loss=288.84207707448576, w0=52.59355358417605, w1=9.67264350007789\n",
      "SGD iter. 12/49: loss=236.88540112541122, w0=54.66359042596896, w1=10.053350393569003\n",
      "SGD iter. 13/49: loss=194.80049360666067, w0=56.526623583582584, w1=10.395986597711005\n",
      "SGD iter. 14/49: loss=160.71171851647279, w0=58.20335342543484, w1=10.704359181438807\n",
      "SGD iter. 15/49: loss=133.09981069342058, w0=59.712410283101875, w1=10.98189450679383\n",
      "SGD iter. 16/49: loss=110.73416535674828, w0=61.070561455002206, w1=11.231676299613351\n",
      "SGD iter. 17/49: loss=92.61799263404367, w0=62.2928975097125, w1=11.45647991315092\n",
      "SGD iter. 18/49: loss=77.943892728653, w0=63.39299995895177, w1=11.658803165334731\n",
      "SGD iter. 19/49: loss=66.05787180528654, w0=64.38309216326711, w1=11.840894092300163\n",
      "SGD iter. 20/49: loss=56.430194857359716, w0=65.27417514715091, w1=12.00477592656905\n",
      "SGD iter. 21/49: loss=48.63177652953898, w0=66.07614983264634, w1=12.15226957741105\n",
      "SGD iter. 22/49: loss=42.31505768400419, w0=66.79792704959222, w1=12.285013863168848\n",
      "SGD iter. 23/49: loss=37.19851541912101, w0=67.44752654484351, w1=12.404483720350868\n",
      "SGD iter. 24/49: loss=33.054116184565615, w0=68.03216609056967, w1=12.512006591814686\n",
      "SGD iter. 25/49: loss=29.69715280457577, w0=68.55834168172322, w1=12.608777176132122\n",
      "SGD iter. 26/49: loss=26.978012466783984, w0=69.03189971376142, w1=12.695870702017814\n",
      "SGD iter. 27/49: loss=24.775508793172605, w0=69.45810194259579, w1=12.774254875314936\n",
      "SGD iter. 28/49: loss=22.991480817547426, w0=69.84168394854673, w1=12.844800631282347\n",
      "SGD iter. 29/49: loss=21.546418157290997, w0=70.18690775390257, w1=12.908291811653017\n",
      "SGD iter. 30/49: loss=20.3759174024833, w0=70.49760917872284, w1=12.965433873986619\n",
      "SGD iter. 31/49: loss=19.42781179108905, w0=70.77724046106107, w1=13.01686173008686\n",
      "SGD iter. 32/49: loss=18.65984624585973, w0=71.02890861516548, w1=13.06314680057708\n",
      "SGD iter. 33/49: loss=18.037794154223974, w0=71.25540995385944, w1=13.104803364018277\n",
      "SGD iter. 34/49: loss=17.53393195999902, w0=71.45926115868401, w1=13.142294271115354\n",
      "SGD iter. 35/49: loss=17.1258035826768, w0=71.64272724302613, w1=13.176036087502723\n",
      "SGD iter. 36/49: loss=16.795219597045794, w0=71.80784671893403, w1=13.206403722251356\n",
      "SGD iter. 37/49: loss=16.527446568684688, w0=71.95645424725114, w1=13.233734593525124\n",
      "SGD iter. 38/49: loss=16.310550415712186, w0=72.09020102273655, w1=13.258332377671517\n",
      "SGD iter. 39/49: loss=16.134864531804457, w0=72.21057312067342, w1=13.28047038340327\n",
      "SGD iter. 40/49: loss=15.992558965839189, w0=72.31890800881659, w1=13.300394588561847\n",
      "SGD iter. 41/49: loss=15.877291457407333, w0=72.41640940814546, w1=13.318326373204567\n",
      "SGD iter. 42/49: loss=15.783924775577523, w0=72.50416066754143, w1=13.334464979383014\n",
      "SGD iter. 43/49: loss=15.70829776329538, w0=72.5831368009978, w1=13.348989724943618\n",
      "SGD iter. 44/49: loss=15.64703988334684, w0=72.65421532110854, w1=13.36206199594816\n",
      "SGD iter. 45/49: loss=15.597421000588529, w0=72.71818598920821, w1=13.373827039852248\n",
      "SGD iter. 46/49: loss=15.557229705554294, w0=72.77575959049791, w1=13.384415579365928\n",
      "SGD iter. 47/49: loss=15.52467475657656, w0=72.82757583165863, w1=13.39394526492824\n",
      "SGD iter. 48/49: loss=15.498305247904602, w0=72.8742104487033, w1=13.40252198193432\n",
      "SGD iter. 49/49: loss=15.476945945880312, w0=72.91618160404349, w1=13.410241027239794\n",
      "SGD: execution time=0.049 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf91b18a3dd4b3daf3b32db99356c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=318.282124701595, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165126, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.97477639885521, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260339, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=65.93073010260336, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989095\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.008 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "\n",
    "sgd_losses, sgd_ws = gradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma\n",
    ")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4632820b41554214ae8b3d97b2d2bea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    e = (y - tx @ w)\n",
    "    return - tx.T @ np.sign(e)/ len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        grad = compute_subgradient_mae(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        \n",
    "       \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w  - gamma*grad\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=2869.8351145358524, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=2818.2326504374046, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=2767.120186338956, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=2716.4977222405073, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=2666.365258142059, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=2616.72279404361, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=2567.570329945162, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=2518.9078658467133, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=2470.735401748265, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=2423.052937649817, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=2375.860473551368, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=2329.1580094529195, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=2282.9455453544715, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=2237.223081256023, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=2191.9906171575744, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=2147.2481530591263, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=2102.995688960677, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=2059.2332248622292, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=2015.9607607637806, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=1973.1782966653323, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=1930.8858325668837, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=1889.0833684684353, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=1847.7709043699867, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=1806.9484402715382, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=1766.6159761730898, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=1726.7735120746415, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=1687.421047976193, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=1648.5585838777445, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=1610.1861197792962, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=1572.3036556808477, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=1534.9111915823992, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=1498.0087274839507, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=1461.5962633855022, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=1425.6737992870537, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=1390.2413351886055, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=1355.2988710901568, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=1320.8464069917086, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=1286.8839428932602, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=1253.4114787948113, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=1220.429014696363, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=1187.9365505979147, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=1155.9340864994665, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=1124.4216224010179, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=1093.3991583025693, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=1062.8666942041207, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=1032.8242301056723, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=1003.2717660072237, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=974.2093019087752, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=945.6368378103266, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=917.5543737118779, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=889.9619096134294, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=862.8594455149807, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=836.2469814165323, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=810.1245173180836, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=784.4920532196351, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=759.3495891211865, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=734.6971250227379, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=710.5346609242895, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=686.8621968258408, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=663.6797327273923, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=640.9872686289436, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=618.7848045304952, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=597.0723404320465, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=575.8498763335981, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=555.1174122351496, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=534.8749481367009, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=515.1224840382524, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=495.8600199398039, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=477.14806692939726, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=458.9765238171776, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=441.2762481736446, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=424.24408268142975, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=407.6944527790816, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=391.5821885242347, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=375.99555288487284, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=360.9569535092958, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=346.3748247543326, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=332.3117701076253, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=318.6833724768495, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=305.5086034302326, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=292.76667341735026, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=280.53153011615797, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=268.8966841753539, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=257.7339200525458, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=246.93766902970742, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=236.6435234459223, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=226.7515498304498, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=217.35738896411294, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=208.28322256993147, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=199.60239149197486, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=191.5160682372005, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=183.75485658516752, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=176.30486084041334, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=169.100122264671, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=162.25177552382942, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=155.6974229971434, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=149.527526794962, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=143.6406908762161, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=138.01748857628547, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=132.61620926126307, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=127.54972442477158, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=122.7408733871857, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=118.14592856152603, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=113.76488994779254, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=109.59775754598526, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=105.79833542751524, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=102.29523108809977, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=98.99125186585265, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=95.97103181808454, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=93.14678297619835, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=90.61539672531048, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=88.27117995547901, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=86.11413266670394, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=84.16459694644125, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=82.46374060728382, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=80.92130656136145, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=79.52815785669324, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=78.31663397216155, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=77.31763568851031, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=76.50863231252772, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=75.84369059931622, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=75.29728112325216, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=74.79581982001427, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=74.43521733660026, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=74.19719822092485, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=74.06837899395498, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=74.03676697743124, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=74.08918974672692, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=74.22098311709007, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=74.41647628166024, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=74.67096152833813, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=74.95294838238388, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=75.17779670886824, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=75.4215490289155, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=75.65853052170146, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=75.90956114411351, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=76.17464089615169, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=76.41613751021141, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=76.65285618006462, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=76.89760893143634, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=77.10246868795771, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=77.27462217352515, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=77.42392987125342, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=77.56681600428637, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=77.6575549725318, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=77.69773565765118, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=77.77686805360935, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=77.80488113813034, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=77.8334888203511, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=77.86269110027165, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=77.9441777135799, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=77.97453880885702, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=78.01721470220257, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=78.06006252205766, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=78.1030822684223, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=78.12180591760107, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=78.14054518714481, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=78.15930007705352, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=78.17807058732721, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=78.19685671796589, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=78.21565846896952, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=78.23447584033816, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=78.25330883207178, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=78.27215744417036, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=78.29102167663392, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=78.30990152946246, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=78.32879700265599, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=78.34770809621449, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=78.36663481013798, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=78.38557714442643, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=78.40453509907987, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=78.42350867409829, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=78.44249786948167, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=78.46150268523004, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=78.48052312134341, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=78.49955917782175, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=78.51861085466504, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=78.53767815187334, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=78.5567610694466, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=78.57585960738484, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=78.59497376568808, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=78.6141035443563, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=78.63324894338946, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=78.6524099627876, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=78.67158660255075, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=78.69077886267888, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=78.70998674317197, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=78.72921024403004, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=78.74844936525311, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=78.76770410684114, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=78.78697446879414, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=78.78623350545445, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=78.80551108487171, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=78.82480428465396, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=78.82409907031953, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=78.84339948756605, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=78.86271552517753, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=78.88204718315401, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=78.88136931492416, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=78.90070819036488, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=78.9200626861706, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=78.91942056694602, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=78.93878228021597, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=78.95815961385092, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=78.97755256785086, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=78.97693779473083, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=78.99633796619504, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=79.01575375802422, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=79.01517473390946, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=79.0345977432029, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=79.0540363728613, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=79.07349062288469, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=79.07293894487452, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=79.09240041236217, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=79.11187750021479, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=79.11136157120993, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=79.12342941191532, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=79.12290850230906, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=79.13498681139072, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=79.13446092118305, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=79.146549698641, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=79.1460188278319, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=79.1581180736661, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=79.15758222225561, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=79.14963612667178, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=79.16173864779171, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=79.16120123807913, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=79.1532539627117, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=79.16535975911734, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=79.16482079110266, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=79.15687233595163, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=79.14892647180822, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=79.16102835040903, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=79.16049124639157, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=79.15254420246457, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=79.16464935635108, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=79.16411069403154, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=79.15616247032094, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=79.16827089949315, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=79.16773067887151, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=79.15978127537731, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=79.15183446289075, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=79.16393897425812, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=79.16340061763371, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=79.15545262536352, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=79.16756041201661, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=79.16702049709012, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=79.15907132503634, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=79.1511247439902, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=79.16322861283845, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=79.16269056190917, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=79.15474280107942, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=79.1668499452134, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=79.16631033598202, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=79.15836139536867, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=79.15041504576297, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=79.16251827209209, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=79.16198052685793, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=79.15403299746862, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=79.16613949908346, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=79.16560019554723, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=79.1576514863743, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=79.14970536820903, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=79.16180795201902, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=79.16127051248002, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=79.15332321453113, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=79.16542907362683, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=79.16489007578572, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=79.15694159805324, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=79.14899571132841, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=79.16109765261925, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=79.16056051877536, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=79.15261345226692, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=79.16471866884348, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=79.16417997669753, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=79.15623173040548, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=79.16834022226776, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=79.16779997181969, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=79.15985054574402, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=79.15190371067601, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=79.16400828473344, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=79.16346989828261, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=79.15552188343098, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=79.16762973277415, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=79.16708978802123, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=79.159140593386, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=79.1511939897584, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=79.16329792129669, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=79.16275984054101, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=79.15481205712982, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=79.16691926395383, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=79.16637962489604, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=79.15843066170125, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=79.1504842895141, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=79.16258757853325, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=79.1620498034727, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=79.15410225150194, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=79.16620881580681, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=79.16566948244416, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=79.1577207506898, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=79.1497746099431, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=79.16187725644309, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=79.16133978707768, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=79.15339246654736, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=79.1654983883331, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=79.16495936066559, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=79.15701086035166, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=79.14906495104536, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=79.16116695502626, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=79.16062979135597, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=79.15268270226608, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=79.16478798153268, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=79.16424925956031, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=79.1563009906868, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=79.16840954523911, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=79.16786926496468, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=79.15991981630755, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=79.15197295865809, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=79.16407759540557, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=79.16353917912832, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=79.15559114169525, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=79.16769905372846, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=79.16715907914913, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=79.15920986193244, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=79.1512632357234, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=79.16336722995175, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=79.16282911936965, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=79.15488131337699, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=79.16698858289105, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=79.16644891400686, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=79.15849992823063, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=79.15055353346202, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=79.16265688517124, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=79.16211908028427, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=79.15417150573204, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=79.16627813272696, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=79.16573876953792, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=79.1577900152021, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=79.14984385187392, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=79.161946561064, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=79.16140906187216, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=79.1534617187604, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=79.16556770323619, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=79.16502864574227, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=79.15708012284688, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=79.14913419095913, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=79.1612362576301, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=79.16069906413338, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=79.15275195246204, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=79.1648572944187, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=79.1643185426199, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=79.15637025116494, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=79.16847886840733, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=79.16793855830643, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=79.1599890870679, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=79.15204220683698, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=79.16414690627448, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=79.16360846017085, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=79.15566040015631, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=79.16776837487956, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=79.16722837047381, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=79.1592791306757, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=79.1513324818852, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=79.16343653880361, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=79.16289839839509, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=79.154950569821, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=79.1670579020251, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=79.16651820331448, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=79.1585691949568, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=79.15062277760674, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=79.16272619200603, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=79.16218835729262, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=79.15424076015897, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=79.16634744984394, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=79.16580805682847, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=79.1578592799112, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=79.14991309400158, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=79.16201586588173, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=79.16147833686345, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=79.15353097117023, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=79.16563701833607, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=79.16509793101574, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=79.1571493855389, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=79.1492034310697, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=79.16130556043072, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=79.1607683371076, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=79.1528212028548, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=79.16492660750151, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=79.1643878258763, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=79.1564395118399, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=79.16854819177233, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=79.16800785184503, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=79.16005835802503, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=79.15211145521265, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=79.16421621734025, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=79.16367774141017, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=79.1557296588142, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=79.1678376962275, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=79.16729766199533, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=79.15934839961575, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=79.15140172824383, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=79.16350584785228, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=79.16296767761733, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=79.15501982646178, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=79.16712722135595, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=79.16658749281892, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=79.15863846187979, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=79.15069202194827, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=79.1627954990376, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=79.16225763449779, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=79.1543100147827, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=79.16641676715771, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=79.16587734431583, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=79.15792854481711, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=79.14998233632603, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=79.16208517089623, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=79.16154761205155, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=79.15360022377686, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=79.16570633363277, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=79.16516721648603, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=79.15721864842774, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=79.1492726713771, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=79.16137486342815, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=79.1608376102786, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=79.15289045344437, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=79.16499592078114, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=79.16445710932952, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=79.15650877271166, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=79.16861751533413, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=79.16807714558041, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=79.16012762917896, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=79.15218070378513, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=79.16428552860279, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=79.16374702284628, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=79.15579891766887, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=79.16790701777222, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=79.16736695371364, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=79.15941766875262, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=79.15147097479922, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=79.16357515709774, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=79.16303695703638, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=79.1550890832994, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=79.16719654088361, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=79.16665678252016, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=79.15870772899957, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=79.1507612664866, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=79.16286480626599, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=79.16232691189977, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=79.1543792696032, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=79.16648608466829, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=79.16594663199997, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=79.15799780991982, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=79.1500515788473, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=79.16215447610753, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=79.16161688743645, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=79.15366947658032, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=79.16577564912629, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=79.16523650215309, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=79.15728791151336, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=79.14934191188128, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=79.16144416662237, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=79.16090688364643, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=79.15295970423074, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=79.16506523425757, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=79.16452639297951, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=79.1565780337802, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=79.16868683909274, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=79.16814643951261, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=79.16019690052971, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=79.15224995255444, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=79.16435484006213, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=79.16381630447921, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=79.15586817672035, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=79.16797633951374, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=79.16743624562875, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=79.15948693808627, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=79.15154022155146, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=79.16364446654, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=79.16310623665223, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=79.1551583403338, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=79.16726586060807, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=79.1667260724182, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=79.15877699631616, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=79.15083051122177, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=79.16293411369118, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=79.16239618949854, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=79.15444852462053, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=79.16655540237568, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=79.16601591988096, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=79.15806707521934, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=79.15012082156535, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=79.16222378151568, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=79.16168616301816, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=79.15373872958057, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=79.16584496481659, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=79.16530578801698, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=79.1573571747958, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=79.14941115258226, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=79.16151347001343, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=79.16097615721105, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.032 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806d7a589ca449a591bc1dbdd1f8a041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        grad = compute_subgradient_mae(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        \n",
    "       \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w  - gamma*grad\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=2869.8351145358524, w0=0.7, w1=8.756471895211877e-16\n",
      "SubSGD iter. 1/499: loss=2818.2326504374046, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubSGD iter. 2/499: loss=2767.120186338956, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubSGD iter. 3/499: loss=2716.4977222405073, w0=2.8, w1=3.502588758084751e-15\n",
      "SubSGD iter. 4/499: loss=2666.365258142059, w0=3.5, w1=4.378235947605939e-15\n",
      "SubSGD iter. 5/499: loss=2616.72279404361, w0=4.2, w1=5.253883137127127e-15\n",
      "SubSGD iter. 6/499: loss=2567.570329945162, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubSGD iter. 7/499: loss=2518.9078658467133, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubSGD iter. 8/499: loss=2470.735401748265, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubSGD iter. 9/499: loss=2423.052937649817, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubSGD iter. 10/499: loss=2375.860473551368, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubSGD iter. 11/499: loss=2329.1580094529195, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubSGD iter. 12/499: loss=2282.9455453544715, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubSGD iter. 13/499: loss=2237.223081256023, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubSGD iter. 14/499: loss=2191.9906171575744, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubSGD iter. 15/499: loss=2147.2481530591263, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubSGD iter. 16/499: loss=2102.995688960677, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubSGD iter. 17/499: loss=2059.2332248622292, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubSGD iter. 18/499: loss=2015.9607607637806, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubSGD iter. 19/499: loss=1973.1782966653323, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubSGD iter. 20/499: loss=1930.8858325668837, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubSGD iter. 21/499: loss=1889.0833684684353, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubSGD iter. 22/499: loss=1847.7709043699867, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubSGD iter. 23/499: loss=1806.9484402715382, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubSGD iter. 24/499: loss=1766.6159761730898, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubSGD iter. 25/499: loss=1726.7735120746415, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubSGD iter. 26/499: loss=1687.421047976193, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubSGD iter. 27/499: loss=1648.5585838777445, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubSGD iter. 28/499: loss=1610.1861197792962, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubSGD iter. 29/499: loss=1572.3036556808477, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubSGD iter. 30/499: loss=1534.9111915823992, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubSGD iter. 31/499: loss=1498.0087274839507, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubSGD iter. 32/499: loss=1461.5962633855022, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubSGD iter. 33/499: loss=1425.6737992870537, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubSGD iter. 34/499: loss=1390.2413351886055, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubSGD iter. 35/499: loss=1355.2988710901568, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubSGD iter. 36/499: loss=1320.8464069917086, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubSGD iter. 37/499: loss=1286.8839428932602, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubSGD iter. 38/499: loss=1253.4114787948113, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubSGD iter. 39/499: loss=1220.429014696363, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubSGD iter. 40/499: loss=1187.9365505979147, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubSGD iter. 41/499: loss=1155.9340864994665, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubSGD iter. 42/499: loss=1124.4216224010179, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubSGD iter. 43/499: loss=1093.3991583025693, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubSGD iter. 44/499: loss=1062.8666942041207, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubSGD iter. 45/499: loss=1032.8242301056723, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubSGD iter. 46/499: loss=1003.2717660072237, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubSGD iter. 47/499: loss=974.2093019087752, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubSGD iter. 48/499: loss=945.6368378103266, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubSGD iter. 49/499: loss=917.5543737118779, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubSGD iter. 50/499: loss=889.9619096134294, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubSGD iter. 51/499: loss=862.8594455149807, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubSGD iter. 52/499: loss=836.2469814165323, w0=37.1, w1=4.640930104462295e-14\n",
      "SubSGD iter. 53/499: loss=810.1245173180836, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubSGD iter. 54/499: loss=784.4920532196351, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubSGD iter. 55/499: loss=759.3495891211865, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubSGD iter. 56/499: loss=734.6971250227379, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubSGD iter. 57/499: loss=710.5346609242895, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubSGD iter. 58/499: loss=686.8621968258408, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubSGD iter. 59/499: loss=663.6797327273923, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubSGD iter. 60/499: loss=640.9872686289436, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubSGD iter. 61/499: loss=618.7848045304952, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubSGD iter. 62/499: loss=597.0723404320465, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubSGD iter. 63/499: loss=575.8498763335981, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubSGD iter. 64/499: loss=555.1174122351496, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubSGD iter. 65/499: loss=534.8749481367009, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubSGD iter. 66/499: loss=515.1224840382524, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubSGD iter. 67/499: loss=495.8600199398039, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubSGD iter. 68/499: loss=477.14806692939726, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubSGD iter. 69/499: loss=458.9765238171776, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubSGD iter. 70/499: loss=441.2762481736446, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubSGD iter. 71/499: loss=424.24408268142975, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubSGD iter. 72/499: loss=407.6944527790816, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubSGD iter. 73/499: loss=391.5821885242347, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubSGD iter. 74/499: loss=375.99555288487284, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubSGD iter. 75/499: loss=360.9569535092958, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubSGD iter. 76/499: loss=346.3748247543326, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubSGD iter. 77/499: loss=332.3117701076253, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubSGD iter. 78/499: loss=318.6833724768495, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubSGD iter. 79/499: loss=305.5086034302326, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubSGD iter. 80/499: loss=292.76667341735026, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubSGD iter. 81/499: loss=280.53153011615797, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubSGD iter. 82/499: loss=268.8966841753539, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubSGD iter. 83/499: loss=257.7339200525458, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubSGD iter. 84/499: loss=246.93766902970742, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubSGD iter. 85/499: loss=236.6435234459223, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubSGD iter. 86/499: loss=226.7515498304498, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubSGD iter. 87/499: loss=217.35738896411294, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubSGD iter. 88/499: loss=208.28322256993147, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubSGD iter. 89/499: loss=199.60239149197486, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubSGD iter. 90/499: loss=191.5160682372005, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubSGD iter. 91/499: loss=183.75485658516752, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubSGD iter. 92/499: loss=176.30486084041334, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubSGD iter. 93/499: loss=169.100122264671, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubSGD iter. 94/499: loss=162.25177552382942, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubSGD iter. 95/499: loss=155.6974229971434, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubSGD iter. 96/499: loss=149.527526794962, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubSGD iter. 97/499: loss=143.6406908762161, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubSGD iter. 98/499: loss=138.01748857628547, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubSGD iter. 99/499: loss=132.61620926126307, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubSGD iter. 100/499: loss=127.54972442477158, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubSGD iter. 101/499: loss=122.7408733871857, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubSGD iter. 102/499: loss=118.14592856152603, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubSGD iter. 103/499: loss=113.76488994779254, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubSGD iter. 104/499: loss=109.59775754598526, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubSGD iter. 105/499: loss=105.79833542751524, w0=66.070297029703, w1=8.073669686866932\n",
      "SubSGD iter. 106/499: loss=102.29523108809977, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubSGD iter. 107/499: loss=98.99125186585265, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubSGD iter. 108/499: loss=95.97103181808454, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubSGD iter. 109/499: loss=93.14678297619835, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubSGD iter. 110/499: loss=90.61539672531048, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubSGD iter. 111/499: loss=88.27117995547901, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubSGD iter. 112/499: loss=86.11413266670394, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubSGD iter. 113/499: loss=84.16459694644125, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubSGD iter. 114/499: loss=82.46374060728382, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubSGD iter. 115/499: loss=80.92130656136145, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubSGD iter. 116/499: loss=79.52815785669324, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubSGD iter. 117/499: loss=78.31663397216155, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubSGD iter. 118/499: loss=77.31763568851031, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubSGD iter. 119/499: loss=76.50863231252772, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubSGD iter. 120/499: loss=75.84369059931622, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubSGD iter. 121/499: loss=75.29728112325216, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubSGD iter. 122/499: loss=74.79581982001427, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubSGD iter. 123/499: loss=74.43521733660026, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubSGD iter. 124/499: loss=74.19719822092485, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubSGD iter. 125/499: loss=74.06837899395498, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubSGD iter. 126/499: loss=74.03676697743124, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubSGD iter. 127/499: loss=74.08918974672692, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubSGD iter. 128/499: loss=74.22098311709007, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubSGD iter. 129/499: loss=74.41647628166024, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubSGD iter. 130/499: loss=74.67096152833813, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubSGD iter. 131/499: loss=74.95294838238388, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubSGD iter. 132/499: loss=75.17779670886824, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubSGD iter. 133/499: loss=75.4215490289155, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubSGD iter. 134/499: loss=75.65853052170146, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubSGD iter. 135/499: loss=75.90956114411351, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubSGD iter. 136/499: loss=76.17464089615169, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubSGD iter. 137/499: loss=76.41613751021141, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubSGD iter. 138/499: loss=76.65285618006462, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubSGD iter. 139/499: loss=76.89760893143634, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubSGD iter. 140/499: loss=77.10246868795771, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubSGD iter. 141/499: loss=77.27462217352515, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubSGD iter. 142/499: loss=77.42392987125342, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubSGD iter. 143/499: loss=77.56681600428637, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubSGD iter. 144/499: loss=77.6575549725318, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubSGD iter. 145/499: loss=77.69773565765118, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubSGD iter. 146/499: loss=77.77686805360935, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubSGD iter. 147/499: loss=77.80488113813034, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubSGD iter. 148/499: loss=77.8334888203511, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubSGD iter. 149/499: loss=77.86269110027165, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubSGD iter. 150/499: loss=77.9441777135799, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubSGD iter. 151/499: loss=77.97453880885702, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubSGD iter. 152/499: loss=78.01721470220257, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubSGD iter. 153/499: loss=78.06006252205766, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubSGD iter. 154/499: loss=78.1030822684223, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubSGD iter. 155/499: loss=78.12180591760107, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubSGD iter. 156/499: loss=78.14054518714481, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubSGD iter. 157/499: loss=78.15930007705352, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubSGD iter. 158/499: loss=78.17807058732721, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubSGD iter. 159/499: loss=78.19685671796589, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubSGD iter. 160/499: loss=78.21565846896952, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubSGD iter. 161/499: loss=78.23447584033816, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubSGD iter. 162/499: loss=78.25330883207178, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubSGD iter. 163/499: loss=78.27215744417036, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubSGD iter. 164/499: loss=78.29102167663392, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubSGD iter. 165/499: loss=78.30990152946246, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubSGD iter. 166/499: loss=78.32879700265599, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubSGD iter. 167/499: loss=78.34770809621449, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubSGD iter. 168/499: loss=78.36663481013798, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubSGD iter. 169/499: loss=78.38557714442643, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubSGD iter. 170/499: loss=78.40453509907987, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubSGD iter. 171/499: loss=78.42350867409829, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubSGD iter. 172/499: loss=78.44249786948167, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubSGD iter. 173/499: loss=78.46150268523004, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubSGD iter. 174/499: loss=78.48052312134341, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubSGD iter. 175/499: loss=78.49955917782175, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubSGD iter. 176/499: loss=78.51861085466504, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubSGD iter. 177/499: loss=78.53767815187334, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubSGD iter. 178/499: loss=78.5567610694466, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubSGD iter. 179/499: loss=78.57585960738484, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubSGD iter. 180/499: loss=78.59497376568808, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubSGD iter. 181/499: loss=78.6141035443563, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubSGD iter. 182/499: loss=78.63324894338946, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubSGD iter. 183/499: loss=78.6524099627876, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubSGD iter. 184/499: loss=78.67158660255075, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubSGD iter. 185/499: loss=78.69077886267888, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubSGD iter. 186/499: loss=78.70998674317197, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubSGD iter. 187/499: loss=78.72921024403004, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubSGD iter. 188/499: loss=78.74844936525311, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubSGD iter. 189/499: loss=78.76770410684114, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubSGD iter. 190/499: loss=78.78697446879414, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubSGD iter. 191/499: loss=78.78623350545445, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubSGD iter. 192/499: loss=78.80551108487171, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubSGD iter. 193/499: loss=78.82480428465396, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubSGD iter. 194/499: loss=78.82409907031953, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubSGD iter. 195/499: loss=78.84339948756605, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubSGD iter. 196/499: loss=78.86271552517753, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubSGD iter. 197/499: loss=78.88204718315401, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubSGD iter. 198/499: loss=78.88136931492416, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubSGD iter. 199/499: loss=78.90070819036488, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubSGD iter. 200/499: loss=78.9200626861706, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubSGD iter. 201/499: loss=78.91942056694602, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubSGD iter. 202/499: loss=78.93878228021597, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubSGD iter. 203/499: loss=78.95815961385092, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubSGD iter. 204/499: loss=78.97755256785086, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubSGD iter. 205/499: loss=78.97693779473083, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubSGD iter. 206/499: loss=78.99633796619504, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubSGD iter. 207/499: loss=79.01575375802422, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubSGD iter. 208/499: loss=79.01517473390946, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubSGD iter. 209/499: loss=79.0345977432029, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubSGD iter. 210/499: loss=79.0540363728613, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubSGD iter. 211/499: loss=79.07349062288469, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubSGD iter. 212/499: loss=79.07293894487452, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubSGD iter. 213/499: loss=79.09240041236217, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubSGD iter. 214/499: loss=79.11187750021479, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubSGD iter. 215/499: loss=79.11136157120993, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubSGD iter. 216/499: loss=79.12342941191532, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubSGD iter. 217/499: loss=79.12290850230906, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubSGD iter. 218/499: loss=79.13498681139072, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubSGD iter. 219/499: loss=79.13446092118305, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubSGD iter. 220/499: loss=79.146549698641, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubSGD iter. 221/499: loss=79.1460188278319, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubSGD iter. 222/499: loss=79.1581180736661, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubSGD iter. 223/499: loss=79.15758222225561, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubSGD iter. 224/499: loss=79.14963612667178, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubSGD iter. 225/499: loss=79.16173864779171, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubSGD iter. 226/499: loss=79.16120123807913, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubSGD iter. 227/499: loss=79.1532539627117, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubSGD iter. 228/499: loss=79.16535975911734, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubSGD iter. 229/499: loss=79.16482079110266, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubSGD iter. 230/499: loss=79.15687233595163, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubSGD iter. 231/499: loss=79.14892647180822, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubSGD iter. 232/499: loss=79.16102835040903, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubSGD iter. 233/499: loss=79.16049124639157, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubSGD iter. 234/499: loss=79.15254420246457, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubSGD iter. 235/499: loss=79.16464935635108, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubSGD iter. 236/499: loss=79.16411069403154, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubSGD iter. 237/499: loss=79.15616247032094, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubSGD iter. 238/499: loss=79.16827089949315, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubSGD iter. 239/499: loss=79.16773067887151, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubSGD iter. 240/499: loss=79.15978127537731, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubSGD iter. 241/499: loss=79.15183446289075, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubSGD iter. 242/499: loss=79.16393897425812, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubSGD iter. 243/499: loss=79.16340061763371, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubSGD iter. 244/499: loss=79.15545262536352, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubSGD iter. 245/499: loss=79.16756041201661, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubSGD iter. 246/499: loss=79.16702049709012, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubSGD iter. 247/499: loss=79.15907132503634, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubSGD iter. 248/499: loss=79.1511247439902, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubSGD iter. 249/499: loss=79.16322861283845, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubSGD iter. 250/499: loss=79.16269056190917, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubSGD iter. 251/499: loss=79.15474280107942, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubSGD iter. 252/499: loss=79.1668499452134, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubSGD iter. 253/499: loss=79.16631033598202, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubSGD iter. 254/499: loss=79.15836139536867, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubSGD iter. 255/499: loss=79.15041504576297, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubSGD iter. 256/499: loss=79.16251827209209, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubSGD iter. 257/499: loss=79.16198052685793, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubSGD iter. 258/499: loss=79.15403299746862, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubSGD iter. 259/499: loss=79.16613949908346, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubSGD iter. 260/499: loss=79.16560019554723, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubSGD iter. 261/499: loss=79.1576514863743, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubSGD iter. 262/499: loss=79.14970536820903, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubSGD iter. 263/499: loss=79.16180795201902, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubSGD iter. 264/499: loss=79.16127051248002, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubSGD iter. 265/499: loss=79.15332321453113, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubSGD iter. 266/499: loss=79.16542907362683, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubSGD iter. 267/499: loss=79.16489007578572, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubSGD iter. 268/499: loss=79.15694159805324, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubSGD iter. 269/499: loss=79.14899571132841, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubSGD iter. 270/499: loss=79.16109765261925, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubSGD iter. 271/499: loss=79.16056051877536, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubSGD iter. 272/499: loss=79.15261345226692, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubSGD iter. 273/499: loss=79.16471866884348, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubSGD iter. 274/499: loss=79.16417997669753, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubSGD iter. 275/499: loss=79.15623173040548, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubSGD iter. 276/499: loss=79.16834022226776, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubSGD iter. 277/499: loss=79.16779997181969, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubSGD iter. 278/499: loss=79.15985054574402, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubSGD iter. 279/499: loss=79.15190371067601, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubSGD iter. 280/499: loss=79.16400828473344, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubSGD iter. 281/499: loss=79.16346989828261, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubSGD iter. 282/499: loss=79.15552188343098, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubSGD iter. 283/499: loss=79.16762973277415, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubSGD iter. 284/499: loss=79.16708978802123, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubSGD iter. 285/499: loss=79.159140593386, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubSGD iter. 286/499: loss=79.1511939897584, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubSGD iter. 287/499: loss=79.16329792129669, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubSGD iter. 288/499: loss=79.16275984054101, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubSGD iter. 289/499: loss=79.15481205712982, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubSGD iter. 290/499: loss=79.16691926395383, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubSGD iter. 291/499: loss=79.16637962489604, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubSGD iter. 292/499: loss=79.15843066170125, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubSGD iter. 293/499: loss=79.1504842895141, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubSGD iter. 294/499: loss=79.16258757853325, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubSGD iter. 295/499: loss=79.1620498034727, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubSGD iter. 296/499: loss=79.15410225150194, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubSGD iter. 297/499: loss=79.16620881580681, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubSGD iter. 298/499: loss=79.16566948244416, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubSGD iter. 299/499: loss=79.1577207506898, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubSGD iter. 300/499: loss=79.1497746099431, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubSGD iter. 301/499: loss=79.16187725644309, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubSGD iter. 302/499: loss=79.16133978707768, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubSGD iter. 303/499: loss=79.15339246654736, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubSGD iter. 304/499: loss=79.1654983883331, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubSGD iter. 305/499: loss=79.16495936066559, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubSGD iter. 306/499: loss=79.15701086035166, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubSGD iter. 307/499: loss=79.14906495104536, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubSGD iter. 308/499: loss=79.16116695502626, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubSGD iter. 309/499: loss=79.16062979135597, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubSGD iter. 310/499: loss=79.15268270226608, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubSGD iter. 311/499: loss=79.16478798153268, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubSGD iter. 312/499: loss=79.16424925956031, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubSGD iter. 313/499: loss=79.1563009906868, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubSGD iter. 314/499: loss=79.16840954523911, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubSGD iter. 315/499: loss=79.16786926496468, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubSGD iter. 316/499: loss=79.15991981630755, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubSGD iter. 317/499: loss=79.15197295865809, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubSGD iter. 318/499: loss=79.16407759540557, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubSGD iter. 319/499: loss=79.16353917912832, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubSGD iter. 320/499: loss=79.15559114169525, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubSGD iter. 321/499: loss=79.16769905372846, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubSGD iter. 322/499: loss=79.16715907914913, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubSGD iter. 323/499: loss=79.15920986193244, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubSGD iter. 324/499: loss=79.1512632357234, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubSGD iter. 325/499: loss=79.16336722995175, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubSGD iter. 326/499: loss=79.16282911936965, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubSGD iter. 327/499: loss=79.15488131337699, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubSGD iter. 328/499: loss=79.16698858289105, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubSGD iter. 329/499: loss=79.16644891400686, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubSGD iter. 330/499: loss=79.15849992823063, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubSGD iter. 331/499: loss=79.15055353346202, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubSGD iter. 332/499: loss=79.16265688517124, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubSGD iter. 333/499: loss=79.16211908028427, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubSGD iter. 334/499: loss=79.15417150573204, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubSGD iter. 335/499: loss=79.16627813272696, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubSGD iter. 336/499: loss=79.16573876953792, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubSGD iter. 337/499: loss=79.1577900152021, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubSGD iter. 338/499: loss=79.14984385187392, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubSGD iter. 339/499: loss=79.161946561064, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubSGD iter. 340/499: loss=79.16140906187216, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubSGD iter. 341/499: loss=79.1534617187604, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubSGD iter. 342/499: loss=79.16556770323619, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubSGD iter. 343/499: loss=79.16502864574227, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubSGD iter. 344/499: loss=79.15708012284688, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubSGD iter. 345/499: loss=79.14913419095913, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubSGD iter. 346/499: loss=79.1612362576301, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubSGD iter. 347/499: loss=79.16069906413338, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubSGD iter. 348/499: loss=79.15275195246204, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubSGD iter. 349/499: loss=79.1648572944187, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubSGD iter. 350/499: loss=79.1643185426199, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubSGD iter. 351/499: loss=79.15637025116494, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubSGD iter. 352/499: loss=79.16847886840733, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubSGD iter. 353/499: loss=79.16793855830643, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubSGD iter. 354/499: loss=79.1599890870679, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubSGD iter. 355/499: loss=79.15204220683698, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubSGD iter. 356/499: loss=79.16414690627448, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubSGD iter. 357/499: loss=79.16360846017085, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubSGD iter. 358/499: loss=79.15566040015631, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubSGD iter. 359/499: loss=79.16776837487956, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubSGD iter. 360/499: loss=79.16722837047381, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubSGD iter. 361/499: loss=79.1592791306757, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubSGD iter. 362/499: loss=79.1513324818852, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubSGD iter. 363/499: loss=79.16343653880361, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubSGD iter. 364/499: loss=79.16289839839509, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubSGD iter. 365/499: loss=79.154950569821, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubSGD iter. 366/499: loss=79.1670579020251, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubSGD iter. 367/499: loss=79.16651820331448, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubSGD iter. 368/499: loss=79.1585691949568, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubSGD iter. 369/499: loss=79.15062277760674, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubSGD iter. 370/499: loss=79.16272619200603, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubSGD iter. 371/499: loss=79.16218835729262, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubSGD iter. 372/499: loss=79.15424076015897, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubSGD iter. 373/499: loss=79.16634744984394, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubSGD iter. 374/499: loss=79.16580805682847, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubSGD iter. 375/499: loss=79.1578592799112, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubSGD iter. 376/499: loss=79.14991309400158, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubSGD iter. 377/499: loss=79.16201586588173, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubSGD iter. 378/499: loss=79.16147833686345, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubSGD iter. 379/499: loss=79.15353097117023, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubSGD iter. 380/499: loss=79.16563701833607, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubSGD iter. 381/499: loss=79.16509793101574, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubSGD iter. 382/499: loss=79.1571493855389, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubSGD iter. 383/499: loss=79.1492034310697, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubSGD iter. 384/499: loss=79.16130556043072, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubSGD iter. 385/499: loss=79.1607683371076, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubSGD iter. 386/499: loss=79.1528212028548, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubSGD iter. 387/499: loss=79.16492660750151, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubSGD iter. 388/499: loss=79.1643878258763, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubSGD iter. 389/499: loss=79.1564395118399, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubSGD iter. 390/499: loss=79.16854819177233, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubSGD iter. 391/499: loss=79.16800785184503, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubSGD iter. 392/499: loss=79.16005835802503, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubSGD iter. 393/499: loss=79.15211145521265, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubSGD iter. 394/499: loss=79.16421621734025, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubSGD iter. 395/499: loss=79.16367774141017, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubSGD iter. 396/499: loss=79.1557296588142, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubSGD iter. 397/499: loss=79.1678376962275, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubSGD iter. 398/499: loss=79.16729766199533, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubSGD iter. 399/499: loss=79.15934839961575, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubSGD iter. 400/499: loss=79.15140172824383, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubSGD iter. 401/499: loss=79.16350584785228, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubSGD iter. 402/499: loss=79.16296767761733, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubSGD iter. 403/499: loss=79.15501982646178, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubSGD iter. 404/499: loss=79.16712722135595, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubSGD iter. 405/499: loss=79.16658749281892, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubSGD iter. 406/499: loss=79.15863846187979, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubSGD iter. 407/499: loss=79.15069202194827, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubSGD iter. 408/499: loss=79.1627954990376, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubSGD iter. 409/499: loss=79.16225763449779, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubSGD iter. 410/499: loss=79.1543100147827, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubSGD iter. 411/499: loss=79.16641676715771, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubSGD iter. 412/499: loss=79.16587734431583, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubSGD iter. 413/499: loss=79.15792854481711, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubSGD iter. 414/499: loss=79.14998233632603, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubSGD iter. 415/499: loss=79.16208517089623, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubSGD iter. 416/499: loss=79.16154761205155, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubSGD iter. 417/499: loss=79.15360022377686, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubSGD iter. 418/499: loss=79.16570633363277, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubSGD iter. 419/499: loss=79.16516721648603, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubSGD iter. 420/499: loss=79.15721864842774, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubSGD iter. 421/499: loss=79.1492726713771, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubSGD iter. 422/499: loss=79.16137486342815, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubSGD iter. 423/499: loss=79.1608376102786, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubSGD iter. 424/499: loss=79.15289045344437, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubSGD iter. 425/499: loss=79.16499592078114, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubSGD iter. 426/499: loss=79.16445710932952, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubSGD iter. 427/499: loss=79.15650877271166, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubSGD iter. 428/499: loss=79.16861751533413, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubSGD iter. 429/499: loss=79.16807714558041, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubSGD iter. 430/499: loss=79.16012762917896, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubSGD iter. 431/499: loss=79.15218070378513, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubSGD iter. 432/499: loss=79.16428552860279, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubSGD iter. 433/499: loss=79.16374702284628, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubSGD iter. 434/499: loss=79.15579891766887, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubSGD iter. 435/499: loss=79.16790701777222, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubSGD iter. 436/499: loss=79.16736695371364, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubSGD iter. 437/499: loss=79.15941766875262, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubSGD iter. 438/499: loss=79.15147097479922, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubSGD iter. 439/499: loss=79.16357515709774, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubSGD iter. 440/499: loss=79.16303695703638, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubSGD iter. 441/499: loss=79.1550890832994, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubSGD iter. 442/499: loss=79.16719654088361, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubSGD iter. 443/499: loss=79.16665678252016, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubSGD iter. 444/499: loss=79.15870772899957, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubSGD iter. 445/499: loss=79.1507612664866, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubSGD iter. 446/499: loss=79.16286480626599, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubSGD iter. 447/499: loss=79.16232691189977, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubSGD iter. 448/499: loss=79.1543792696032, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubSGD iter. 449/499: loss=79.16648608466829, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubSGD iter. 450/499: loss=79.16594663199997, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubSGD iter. 451/499: loss=79.15799780991982, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubSGD iter. 452/499: loss=79.1500515788473, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubSGD iter. 453/499: loss=79.16215447610753, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubSGD iter. 454/499: loss=79.16161688743645, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubSGD iter. 455/499: loss=79.15366947658032, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubSGD iter. 456/499: loss=79.16577564912629, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubSGD iter. 457/499: loss=79.16523650215309, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubSGD iter. 458/499: loss=79.15728791151336, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubSGD iter. 459/499: loss=79.14934191188128, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubSGD iter. 460/499: loss=79.16144416662237, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubSGD iter. 461/499: loss=79.16090688364643, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubSGD iter. 462/499: loss=79.15295970423074, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubSGD iter. 463/499: loss=79.16506523425757, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubSGD iter. 464/499: loss=79.16452639297951, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubSGD iter. 465/499: loss=79.1565780337802, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubSGD iter. 466/499: loss=79.16868683909274, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubSGD iter. 467/499: loss=79.16814643951261, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubSGD iter. 468/499: loss=79.16019690052971, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubSGD iter. 469/499: loss=79.15224995255444, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubSGD iter. 470/499: loss=79.16435484006213, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubSGD iter. 471/499: loss=79.16381630447921, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubSGD iter. 472/499: loss=79.15586817672035, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubSGD iter. 473/499: loss=79.16797633951374, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubSGD iter. 474/499: loss=79.16743624562875, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubSGD iter. 475/499: loss=79.15948693808627, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubSGD iter. 476/499: loss=79.15154022155146, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubSGD iter. 477/499: loss=79.16364446654, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubSGD iter. 478/499: loss=79.16310623665223, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubSGD iter. 479/499: loss=79.1551583403338, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubSGD iter. 480/499: loss=79.16726586060807, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubSGD iter. 481/499: loss=79.1667260724182, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubSGD iter. 482/499: loss=79.15877699631616, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubSGD iter. 483/499: loss=79.15083051122177, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubSGD iter. 484/499: loss=79.16293411369118, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubSGD iter. 485/499: loss=79.16239618949854, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubSGD iter. 486/499: loss=79.15444852462053, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubSGD iter. 487/499: loss=79.16655540237568, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubSGD iter. 488/499: loss=79.16601591988096, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubSGD iter. 489/499: loss=79.15806707521934, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubSGD iter. 490/499: loss=79.15012082156535, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubSGD iter. 491/499: loss=79.16222378151568, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubSGD iter. 492/499: loss=79.16168616301816, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubSGD iter. 493/499: loss=79.15373872958057, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubSGD iter. 494/499: loss=79.16584496481659, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubSGD iter. 495/499: loss=79.16530578801698, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubSGD iter. 496/499: loss=79.1573571747958, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubSGD iter. 497/499: loss=79.14941115258226, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubSGD iter. 498/499: loss=79.16151347001343, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubSGD iter. 499/499: loss=79.16097615721105, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubSGD: execution time=0.056 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f93100d8b0461f8dd7157dc4758a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
